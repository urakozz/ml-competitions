{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_is = np.genfromtxt ('train_numeric.csv', delimiter=\",\", skip_header=2001, usecols = (0,), dtype=int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_len = len(train_is)\n",
    "cat_features = 94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 969), (2000, 2140))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = pd.read_csv('train_numeric.csv', index_col=0, skiprows=1, header=None, nrows=2000, dtype=np.float32)\n",
    "num.fillna(0., inplace=True)\n",
    "cat = pd.read_csv('train_categorical_c.csv', index_col=0, skiprows=1, header=None, nrows=2000, dtype=np.float32)\n",
    "cat = cat.div(cat_features,axis='columns')\n",
    "(num.shape, cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 968), (2000,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = pd.concat([cat, num], axis=1)\n",
    "data = num\n",
    "X_valid = data.values[:,:-1]\n",
    "y_valid = data.values[:,-1]\n",
    "(X_valid.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "data_num_chunks = pd.read_csv('train_numeric.csv'      , index_col=0, chunksize=1000, skiprows=2001, dtype=np.float32, header=None)\n",
    "data_cat_chunks = pd.read_csv('train_categorical_c.csv', index_col=0, chunksize=1000, skiprows=2001, dtype=np.float32, header=None)\n",
    "\n",
    "X_pos = []\n",
    "y_pos = []\n",
    "i = -1\n",
    "for rows_n in data_num_chunks:\n",
    "    i+=1\n",
    "    rows_n.fillna(0., inplace=True)\n",
    "    \n",
    "#     rows_c = data_cat_chunks.__next__()\n",
    "#     rows_c = rows_c.div(cat_features, axis='columns')\n",
    "    \n",
    "#     rows = pd.concat([rows_c, rows_n], axis=1)\n",
    "    rows = rows_n\n",
    "    for row in rows.values:\n",
    "        if row[-1] == 0:\n",
    "            continue\n",
    "        X_pos.append(row[:-1])\n",
    "        y_pos.append(row[-1])\n",
    "    if i %100 == 0:\n",
    "        print(i)\n",
    "\n",
    "X_pos = np.array(X_pos).astype(np.float32)\n",
    "y_pos = np.array(y_pos).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6872, 968), (6872,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_pos.shape, y_pos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"valid_pos.pickle\", 'wb') as f:\n",
    "    data = {\n",
    "        \"X_valid\":X_valid,\n",
    "        \"y_valid\":y_valid,\n",
    "        \"X_pos\": X_pos,\n",
    "        \"y_pos\":y_pos \n",
    "    }\n",
    "    pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pickle.load(open(\"valid_pos.pickle\", 'rb'))\n",
    "X_valid = data[\"X_valid\"]\n",
    "y_valid = data[\"y_valid\"]\n",
    "X_pos = data[\"X_pos\"]\n",
    "y_pos = data[\"y_pos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 968), (2000,), (6872, 968), (6872,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_valid.shape, y_valid.shape, X_pos.shape, y_pos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def wb(wshape=[None], bshape=[None], device='/cpu:0'):\n",
    "    with tf.device(device):\n",
    "        w = tf.get_variable(\"w\", wshape, initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        b = tf.get_variable('b', bshape, initializer=tf.constant_initializer(0.0))\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def to_one_hot(y, nlabels=None):\n",
    "    if nlabels == None:\n",
    "        nlabels = np.unique(y).__len__()\n",
    "    return (np.arange(nlabels) == y[:,None]).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 2), (6872, 2))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid_oh = to_one_hot(y_valid, nlabels=2)\n",
    "y_pos_oh = to_one_hot(y_pos, nlabels=2)\n",
    "\n",
    "(y_valid_oh.shape, y_pos_oh.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#idx_pos = np.where(y == 1)[0]\n",
    "# X_test = data_num.iloc[:,1:-1].values[idx_pos]\n",
    "# y_test = y_oh[idx_pos]\n",
    "\n",
    "# idx_neg = np.where(y == 0)[0]\n",
    "# mask = np.random.choice([False, True], len(idx_neg), p=[0.999, 0.001])\n",
    "# idx_neg = idx_neg[mask]\n",
    "# print(idx_neg.shape)\n",
    "\n",
    "# X_neg = data_num.iloc[:,1:-1].values[idx_neg]\n",
    "# y_neg = y_oh[(idx_neg,)]\n",
    "# X_test = np.concatenate((X_test, X_neg), axis=0)\n",
    "# y_test = np.concatenate((y_test, y_neg), axis=0)\n",
    "# print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data (16, 968)\n",
      "layer1 (16, 1024)\n",
      "layer2 (16, 512)\n",
      "layer3 (16, 256)\n",
      "layer6 (16, 2)\n",
      "data (2000, 968)\n",
      "layer1 (2000, 1024)\n",
      "layer2 (2000, 512)\n",
      "layer3 (2000, 256)\n",
      "layer6 (2000, 2)\n",
      "data (6872, 968)\n",
      "layer1 (6872, 1024)\n",
      "layer2 (6872, 512)\n",
      "layer3 (6872, 256)\n",
      "layer6 (6872, 2)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "l2_reg_norm = 5e-5\n",
    "features = X_valid.shape[1]\n",
    "train_size = train_len\n",
    "\n",
    "widest = 1024\n",
    "\n",
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    X_tf = tf.placeholder(tf.float32, shape=(batch_size, features))\n",
    "    y_tf = tf.placeholder(tf.float32, shape=(batch_size,2))\n",
    "    X_valid_tf = tf.constant(X_valid)\n",
    "    X_pos_tf = tf.constant(X_pos)\n",
    "\n",
    "    with tf.variable_scope(\"Layer1\"):\n",
    "            layer1_weights, layer1_biases = wb([features, widest], [widest])\n",
    "    with tf.variable_scope(\"Layer2\"):\n",
    "            layer2_weights, layer2_biases = wb([widest, widest//2], [widest//2])\n",
    "    with tf.variable_scope(\"Layer3\"):\n",
    "            layer3_weights, layer3_biases = wb([widest//2, widest//4], [widest//4])\n",
    "#     with tf.variable_scope(\"Layer4\"):\n",
    "#             layer4_weights, layer4_biases = wb([widest//2, widest//4], [widest//4])\n",
    "#     with tf.variable_scope(\"Layer5\"):\n",
    "#             layer5_weights, layer5_biases = wb([widest//4,64], [64])\n",
    "    with tf.variable_scope(\"Layer6\"):\n",
    "            layer6_weights, layer6_biases = wb([widest//4, 2], [2])\n",
    "\n",
    "    def model(data, train=True):\n",
    "        print(\"data\", data.get_shape())\n",
    "        \n",
    "        layer1 = tf.nn.relu(tf.matmul(data, layer1_weights) + layer1_biases)\n",
    "        print(\"layer1\", layer1.get_shape())\n",
    " \n",
    "            \n",
    "        layer2 = tf.nn.relu(tf.matmul(layer1, layer2_weights) + layer2_biases)\n",
    "        print(\"layer2\", layer2.get_shape())\n",
    "#         if train:\n",
    "#             tf.nn.dropout(layer2, 0.5)\n",
    "            \n",
    "        layer3 = tf.nn.relu(tf.matmul(layer2, layer3_weights) + layer3_biases)\n",
    "        print(\"layer3\", layer3.get_shape())\n",
    "        if train:\n",
    "            tf.nn.dropout(layer3, 0.5)\n",
    "            \n",
    "#         layer4 = tf.nn.relu(tf.matmul(layer3, layer4_weights) + layer4_biases)\n",
    "#         print(\"layer4\", layer4.get_shape())\n",
    "#         if train:\n",
    "#             tf.nn.dropout(layer4, 0.5)\n",
    "            \n",
    "#         layer5 = tf.nn.relu(tf.matmul(layer4, layer5_weights) + layer5_biases)\n",
    "#         print(\"layer5\", layer5.get_shape())\n",
    "        \n",
    "        \n",
    "        layer6 = tf.nn.relu(tf.matmul(layer3, layer6_weights) + layer6_biases)\n",
    "        print(\"layer6\", layer6.get_shape())\n",
    "\n",
    "        return layer6\n",
    "\n",
    "    logits = model(X_tf)\n",
    "\n",
    "    loss_data = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, y_tf))\n",
    "    regularizers = (tf.nn.l2_loss(layer1_weights) + tf.nn.l2_loss(layer1_biases) +\n",
    "                    tf.nn.l2_loss(layer2_weights) + tf.nn.l2_loss(layer2_biases) +\n",
    "                    tf.nn.l2_loss(layer3_weights) + tf.nn.l2_loss(layer3_biases) +\n",
    "#                     tf.nn.l2_loss(layer4_weights) + tf.nn.l2_loss(layer4_biases) + \n",
    "#                     tf.nn.l2_loss(layer5_weights) + tf.nn.l2_loss(layer5_biases) + \n",
    "                    tf.nn.l2_loss(layer6_weights) + tf.nn.l2_loss(layer6_biases))\n",
    "    loss_l2 = l2_reg_norm * regularizers\n",
    "    loss = loss_data + loss_l2\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    learn_rate  = tf.train.exponential_decay(.0001, global_step*batch_size, train_size//4, 0.5, staircase=True)\n",
    "#     tf.scalar_summary('learning_rate', learn_rate)\n",
    "    optimizer = tf.train.AdamOptimizer(learn_rate).minimize(loss, global_step=global_step, name=\"Optimizer\")\n",
    "\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    test_prediction = tf.nn.softmax(model(X_valid_tf, train=False))\n",
    "    pos_prediction = tf.nn.softmax(model(X_pos_tf, train=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98478\n",
      "Initialized valiables\n",
      "0 1.06162 37.5\n",
      "10 0.90111 75.0\n",
      "20 0.879583 75.0\n",
      "30 0.870382 75.0\n",
      "40 0.971903 62.5\n",
      "50 0.913806 75.0\n",
      "60 0.842342 75.0\n",
      "70 0.839794 75.0\n",
      "80 0.89735 75.0\n",
      "90 0.840152 75.0\n",
      "1000 0.688828 75.0\n",
      "2000 0.809892 68.75\n",
      "3000 0.664647 75.0\n",
      "4000 0.69551 75.0\n",
      "5000 0.666096 68.75\n",
      "6000 0.72448 81.25\n",
      "7000 0.764623 68.75\n",
      "8000 0.764364 75.0\n",
      "9000 0.684916 68.75\n",
      "10000 0.741873 75.0\n",
      "10000 test 92.65\n",
      "10000 pos 61.2048894063\n",
      "[[ 0.95250762  0.04749241  1.          0.        ]\n",
      " [ 0.86362064  0.13637935  1.          0.        ]\n",
      " [ 0.94448936  0.05551058  1.          0.        ]\n",
      " [ 0.86985964  0.13014035  1.          0.        ]\n",
      " [ 0.88753378  0.11246624  0.          1.        ]\n",
      " [ 0.09675524  0.90324473  1.          0.        ]\n",
      " [ 0.90772158  0.09227844  1.          0.        ]\n",
      " [ 0.77464962  0.22535041  0.          1.        ]\n",
      " [ 0.70137888  0.29862115  1.          0.        ]\n",
      " [ 0.94448936  0.05551058  1.          0.        ]\n",
      " [ 0.68646419  0.31353584  1.          0.        ]\n",
      " [ 0.86874771  0.1312523   1.          0.        ]\n",
      " [ 0.90853447  0.09146548  1.          0.        ]\n",
      " [ 0.13213593  0.86786413  0.          1.        ]\n",
      " [ 0.81630832  0.1836917   1.          0.        ]\n",
      " [ 0.57575536  0.42424467  0.          1.        ]]\n",
      "11000 0.718842 81.25\n",
      "12000 0.474894 87.5\n",
      "13000 0.380085 93.75\n",
      "14000 0.527395 81.25\n",
      "15000 0.613871 87.5\n",
      "16000 0.330985 93.75\n",
      "17000 0.403567 87.5\n",
      "18000 0.491015 93.75\n",
      "19000 0.403375 87.5\n",
      "20000 0.317537 100.0\n",
      "20000 test 95.3\n",
      "20000 pos 87.8928987194\n",
      "[[ 0.9751544   0.02484553  1.          0.        ]\n",
      " [ 0.74351341  0.25648665  1.          0.        ]\n",
      " [ 0.63830668  0.36169329  1.          0.        ]\n",
      " [ 0.94354779  0.05645215  1.          0.        ]\n",
      " [ 0.10548379  0.89451629  0.          1.        ]\n",
      " [ 0.11871465  0.88128537  0.          1.        ]\n",
      " [ 0.00463975  0.99536031  0.          1.        ]\n",
      " [ 0.94142091  0.05857914  1.          0.        ]\n",
      " [ 0.75803494  0.24196507  1.          0.        ]\n",
      " [ 0.40300319  0.59699678  0.          1.        ]\n",
      " [ 0.97599912  0.02400083  1.          0.        ]\n",
      " [ 0.92510778  0.07489224  1.          0.        ]\n",
      " [ 0.99774629  0.00225372  1.          0.        ]\n",
      " [ 0.9972192   0.00278078  1.          0.        ]\n",
      " [ 0.98174864  0.01825143  1.          0.        ]\n",
      " [ 0.93606883  0.06393121  1.          0.        ]]\n",
      "21000 0.238548 100.0\n",
      "22000 0.233713 100.0\n",
      "23000 0.296495 100.0\n",
      "24000 0.34565 93.75\n",
      "25000 0.213695 100.0\n",
      "26000 0.267135 100.0\n",
      "27000 0.246826 100.0\n",
      "28000 0.280168 93.75\n",
      "29000 0.280225 93.75\n",
      "30000 0.19633 100.0\n",
      "30000 test 97.2\n",
      "30000 pos 91.7054714785\n",
      "[[  9.98322546e-01   1.67749252e-03   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99981523e-01   1.85187309e-05   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99726355e-01   2.73652782e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99998450e-01   1.53429085e-06   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99743402e-01   2.56607571e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  3.58602069e-02   9.64139760e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  2.92900391e-02   9.70710039e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  1.61388167e-03   9.98386145e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.99001324e-01   9.98712610e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  9.81288552e-01   1.87114105e-02   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99796212e-01   2.03834716e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  1.33019045e-01   8.66980970e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.83819187e-01   1.61808673e-02   1.00000000e+00   0.00000000e+00]\n",
      " [  9.97271597e-01   2.72835256e-03   1.00000000e+00   0.00000000e+00]\n",
      " [  9.91311193e-01   8.68881587e-03   1.00000000e+00   0.00000000e+00]\n",
      " [  9.97909606e-01   2.09035096e-03   1.00000000e+00   0.00000000e+00]]\n",
      "31000 0.265777 100.0\n",
      "32000 0.31038 93.75\n",
      "33000 0.233542 100.0\n",
      "34000 0.202354 100.0\n",
      "35000 0.227947 100.0\n",
      "36000 0.192376 100.0\n",
      "37000 0.199439 100.0\n",
      "38000 0.385572 93.75\n",
      "39000 0.213018 100.0\n",
      "40000 0.293188 93.75\n",
      "40000 test 98.35\n",
      "40000 pos 98.4429569267\n",
      "[[  7.26507092e-03   9.92734909e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  2.76577827e-02   9.72342193e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.99910831e-01   8.92177995e-05   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99999285e-01   7.61587444e-07   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99773204e-01   2.26809905e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99830842e-01   1.69135557e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99999523e-01   4.93586469e-07   1.00000000e+00   0.00000000e+00]\n",
      " [  1.82201102e-01   8.17798853e-01   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99830842e-01   1.69135557e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  1.17570743e-01   8.82429242e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.98846054e-01   1.15393323e-03   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99404311e-01   5.95721009e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99901533e-01   9.84324288e-05   1.00000000e+00   0.00000000e+00]\n",
      " [  1.43186599e-02   9.85681415e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.99924064e-01   7.59574032e-05   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99948978e-01   5.10642094e-05   1.00000000e+00   0.00000000e+00]]\n",
      "41000 0.210125 100.0\n",
      "42000 0.182118 100.0\n",
      "43000 0.232891 100.0\n",
      "44000 0.292703 93.75\n",
      "45000 0.197712 100.0\n",
      "46000 0.22549 100.0\n",
      "47000 0.326718 93.75\n",
      "48000 0.202049 100.0\n",
      "49000 0.189535 100.0\n",
      "50000 0.220222 100.0\n",
      "50000 test 99.05\n",
      "50000 pos 98.8067520373\n",
      "[[  9.99837875e-01   1.62145036e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  2.14709178e-01   7.85290778e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.99919176e-01   8.07719916e-05   1.00000000e+00   0.00000000e+00]\n",
      " [  8.92484095e-03   9.91075158e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.99954820e-01   4.51466876e-05   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99991655e-01   8.38170217e-06   1.00000000e+00   0.00000000e+00]\n",
      " [  3.12044322e-01   6.87955737e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.99997377e-01   2.65165386e-06   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99949813e-01   5.01882641e-05   1.00000000e+00   0.00000000e+00]\n",
      " [  9.08646107e-01   9.13539231e-02   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99999881e-01   1.14092565e-07   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99946594e-01   5.34264836e-05   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99713838e-01   2.86197377e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99956965e-01   4.30127366e-05   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99436319e-01   5.63630485e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  4.05148454e-02   9.59485173e-01   0.00000000e+00   1.00000000e+00]]\n",
      "51000 0.208156 100.0\n",
      "52000 0.184046 100.0\n",
      "53000 0.187921 100.0\n",
      "54000 0.18273 100.0\n",
      "55000 0.181595 100.0\n",
      "56000 0.204047 100.0\n",
      "57000 0.199977 100.0\n",
      "58000 0.182672 100.0\n",
      "59000 0.186771 100.0\n",
      "60000 0.320186 93.75\n",
      "60000 test 99.25\n",
      "60000 pos 99.0104772992\n",
      "[[  9.99986529e-01   1.35288028e-05   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99977112e-01   2.29389007e-05   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99998808e-01   1.13757540e-06   1.00000000e+00   0.00000000e+00]\n",
      " [  9.82150555e-01   1.78494584e-02   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99954104e-01   4.58758514e-05   1.00000000e+00   0.00000000e+00]\n",
      " [  5.43415844e-02   9.45658445e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.99981761e-01   1.82160729e-05   1.00000000e+00   0.00000000e+00]\n",
      " [  1.26783192e-01   8.73216808e-01   1.00000000e+00   0.00000000e+00]\n",
      " [  9.94923770e-01   5.07626450e-03   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99992847e-01   7.10449694e-06   1.00000000e+00   0.00000000e+00]\n",
      " [  3.15303053e-03   9.96846974e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  2.15053961e-01   7.84946084e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.99999881e-01   1.45239639e-07   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99995589e-01   4.45464866e-06   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99992251e-01   7.77454807e-06   1.00000000e+00   0.00000000e+00]\n",
      " [  1.22216297e-02   9.87778366e-01   0.00000000e+00   1.00000000e+00]]\n",
      "61000 0.175324 100.0\n",
      "62000 0.17554 100.0\n",
      "63000 0.187909 100.0\n",
      "64000 0.182909 100.0\n",
      "65000 0.17931 100.0\n",
      "66000 0.18518 100.0\n",
      "67000 0.176554 100.0\n",
      "68000 0.202543 100.0\n",
      "69000 0.3722 93.75\n",
      "70000 0.181274 100.0\n",
      "70000 test 99.1\n",
      "70000 pos 99.7817229336\n",
      "[[  9.99894261e-01   1.05742089e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99991536e-01   8.42036661e-06   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99980927e-01   1.91148411e-05   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99992967e-01   7.00518740e-06   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99843001e-01   1.56975468e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  1.28785968e-01   8.71214032e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.99981880e-01   1.80757470e-05   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99980450e-01   1.95578286e-05   1.00000000e+00   0.00000000e+00]\n",
      " [  1.00000000e+00   1.77995378e-08   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99964476e-01   3.55770462e-05   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99843001e-01   1.56975468e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99868751e-01   1.31224850e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  3.98589764e-03   9.96014118e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  5.21308184e-02   9.47869182e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.96160269e-01   3.83969559e-03   1.00000000e+00   0.00000000e+00]\n",
      " [  1.10737011e-02   9.88926291e-01   0.00000000e+00   1.00000000e+00]]\n",
      "71000 0.186661 100.0\n",
      "72000 0.19686 100.0\n",
      "73000 0.177024 100.0\n",
      "74000 0.174069 100.0\n",
      "75000 0.195089 100.0\n",
      "76000 0.591555 93.75\n",
      "77000 0.173144 100.0\n",
      "78000 0.173636 100.0\n",
      "79000 0.177843 100.0\n",
      "80000 0.177282 100.0\n",
      "80000 test 99.05\n",
      "80000 pos 99.8690337602\n",
      "[[  2.36710943e-02   9.76328909e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.99858379e-01   1.41602417e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  9.98053670e-01   1.94635696e-03   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99396682e-01   6.03266701e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  9.75099027e-01   2.49010567e-02   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99871612e-01   1.28362532e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99995708e-01   4.32184515e-06   1.00000000e+00   0.00000000e+00]\n",
      " [  2.20987350e-02   9.77901220e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  2.13085935e-02   9.78691459e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.66540158e-01   3.34598646e-02   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99998808e-01   1.19304786e-06   1.00000000e+00   0.00000000e+00]\n",
      " [  1.24944169e-02   9.87505555e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.74726558e-01   2.52733808e-02   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99518156e-01   4.81905096e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99986649e-01   1.33576405e-05   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99968290e-01   3.17680569e-05   1.00000000e+00   0.00000000e+00]]\n",
      "81000 0.176783 100.0\n",
      "82000 0.179454 100.0\n",
      "83000 0.171552 100.0\n",
      "84000 0.180345 100.0\n",
      "85000 0.184287 100.0\n",
      "86000 0.176802 100.0\n",
      "87000 0.171558 100.0\n",
      "88000 0.17641 100.0\n",
      "89000 0.187833 100.0\n",
      "90000 0.172578 100.0\n",
      "90000 test 99.3\n",
      "90000 pos 99.8690337602\n",
      "[[  9.99944210e-01   5.57926942e-05   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99947906e-01   5.21329166e-05   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99994874e-01   5.07673622e-06   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99932408e-01   6.76394484e-05   1.00000000e+00   0.00000000e+00]\n",
      " [  3.43745425e-02   9.65625405e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.99855757e-01   1.44248857e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  9.77092206e-01   2.29078084e-02   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99898314e-01   1.01642720e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99821961e-01   1.78074741e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99833941e-01   1.65985271e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  3.43745425e-02   9.65625405e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.99998093e-01   1.89272293e-06   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99980807e-01   1.91330764e-05   1.00000000e+00   0.00000000e+00]\n",
      " [  1.48913097e-02   9.85108674e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.99998093e-01   1.91402592e-06   1.00000000e+00   0.00000000e+00]\n",
      " [  8.56093888e-04   9.99143839e-01   0.00000000e+00   1.00000000e+00]]\n",
      "91000 0.253661 93.75\n",
      "92000 0.173916 100.0\n",
      "93000 0.173128 100.0\n",
      "94000 0.333929 93.75\n",
      "95000 0.188141 100.0\n",
      "96000 0.188405 100.0\n",
      "97000 0.17258 100.0\n",
      "98000 0.279455 93.75\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "pos_size = 4\n",
    "batch_size_part = batch_size-pos_size\n",
    "num_steps = train_len//batch_size_part\n",
    "\n",
    "data_num_chunks = pd.read_csv('train_numeric.csv'      , index_col=0, chunksize=batch_size_part, skiprows=2001, dtype=np.float32, header=None)\n",
    "data_cat_chunks = pd.read_csv('train_categorical_c.csv', index_col=0, chunksize=batch_size_part, skiprows=2001, dtype=np.float32, header=None)\n",
    "\n",
    "\n",
    "print(num_steps)\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init_op = tf.initialize_all_variables()\n",
    "    saver = tf.train.Saver()\n",
    "    init_op.run()\n",
    "    print(\"Initialized valiables\")\n",
    "    for i in range(num_steps):\n",
    "        df_n = data_num_chunks.__next__()\n",
    "        df_n.fillna(0, inplace=True)\n",
    "#         df_c = data_cat_chunks.__next__()\n",
    "#         df_c = df_c.div(cat_features, axis='columns')\n",
    "#         df = pd.concat([df_c, df_n], axis=1)\n",
    "        df = df_n\n",
    "        X_ = df.values[:, :-1]\n",
    "        y_ = to_one_hot(df.values[:, -1], nlabels=2)\n",
    "\n",
    "        \n",
    "#         offset = (i * batch_size_part) % (y_train.shape[0] - batch_size_part)\n",
    "#         y_ = y_train[offset:(offset + batch_size_half)]\n",
    "\n",
    "        offset_pos = (i*pos_size) % (len(X_pos) - pos_size)\n",
    "#         add_idx = idx_pos[offset_pos:offset_pos+1]\n",
    "        y__ = y_pos_oh[offset_pos:offset_pos+pos_size]\n",
    "        X__ = X_pos[offset_pos:offset_pos+pos_size, :]\n",
    "\n",
    "        X_ = np.concatenate((X_, X__), axis=0)\n",
    "        y_ = np.concatenate((y_, y__), axis=0)\n",
    "\n",
    "        \n",
    "        p = np.random.permutation(batch_size)\n",
    "        X_ = X_[p]\n",
    "        y_ = y_[p]\n",
    "        \n",
    "        feed_dict = {X_tf : X_, y_tf : y_}\n",
    "        _, l, pred = sess.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (i < 100 and i%10 == 0) or i%1000 == 0:\n",
    "            print(i, l, accuracy(pred, y_ ))\n",
    "        if i>0 and i%10000 == 0:\n",
    "            print(i, \"test\", accuracy(test_prediction.eval(), y_valid_oh))\n",
    "            print(i, \"pos\", accuracy(pos_prediction.eval(), y_pos_oh))\n",
    "            print(np.c_[pred, y_])\n",
    "        if i>0 and i%30000 == 0:\n",
    "            save_path = saver.save(sess, \"bosch_{}.ckpt\".format(i))\n",
    "    save_path = saver.save(sess, \"bosch_end.ckpt\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data_cat = pd.read_csv(\"train_categorical.csv\")\n",
    "#data_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data_num_test = pd.read_csv(\"test_numeric.csv\")\n",
    "# data_num_test.fillna(0., inplace=True)\n",
    "# data_num_test.head()\n",
    "test_ids = np.genfromtxt ('test_numeric.csv', delimiter=\",\", skip_header=1, usecols = (0,), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1183748,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_test_real = data_num_test.iloc[:,1:].values.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data (?, 968)\n",
      "layer1 (?, 1024)\n",
      "layer2 (?, 512)\n",
      "layer3 (?, 256)\n",
      "layer6 (?, 2)\n",
      "Step 0 (50000, 969) 50000\n",
      "Step 1 (50000, 969) 100000\n",
      "Step 2 (50000, 969) 150000\n",
      "Step 3 (50000, 969) 200000\n",
      "Step 4 (50000, 969) 250000\n",
      "Step 5 (50000, 969) 300000\n",
      "Step 6 (50000, 969) 350000\n",
      "Step 7 (50000, 969) 400000\n",
      "Step 8 (50000, 969) 450000\n",
      "Step 9 (50000, 969) 500000\n",
      "Step 10 (50000, 969) 550000\n",
      "Step 11 (50000, 969) 600000\n",
      "Step 12 (50000, 969) 650000\n",
      "Step 13 (50000, 969) 700000\n",
      "Step 14 (50000, 969) 750000\n",
      "Step 15 (50000, 969) 800000\n",
      "Step 16 (50000, 969) 850000\n",
      "Step 17 (50000, 969) 900000\n",
      "Step 18 (50000, 969) 950000\n",
      "Step 19 (50000, 969) 1000000\n",
      "Step 20 (50000, 969) 1050000\n",
      "Step 21 (50000, 969) 1100000\n",
      "Step 22 (50000, 969) 1150000\n",
      "Step 23 (33748, 969) 1183748\n"
     ]
    }
   ],
   "source": [
    "data_real = pd.read_csv('test_numeric.csv', chunksize=50000, skiprows=1, dtype=np.float32, header=None)\n",
    "\n",
    "ans = np.array([])\n",
    "with tf.Session(graph = graph) as sess:\n",
    "    init_op = tf.initialize_all_variables()\n",
    "    saver = tf.train.Saver()\n",
    "    init_op.run()\n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, \"bosch_end.ckpt\")\n",
    "    \n",
    "    X_tf_test = tf.placeholder(tf.float32, shape=(None, features))\n",
    "    test_prediction = tf.nn.softmax(model(X_tf_test, train=False))\n",
    "    \n",
    "    i = -1\n",
    "    for data in data_real:\n",
    "        i+=1\n",
    "        data.fillna(0., inplace=True)\n",
    "        ans_ = sess.run(test_prediction, feed_dict = {X_tf_test:data.values[:, 1:]})\n",
    "        if i == 0:\n",
    "            ans = ans_\n",
    "        else:\n",
    "            ans = np.concatenate((ans, ans_), axis=0)\n",
    "        print(\"Step\", i, data.shape, len(ans))\n",
    "\n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1183748, 2), (1183748,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ans.shape, test_ids.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ans = np.argmax(ans, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = np.c_[test_ids, ans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('answer.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',')\n",
    "    writer.writerow([\"Id\",\"Response\"])\n",
    "    for i, a in enumerate(ans1):\n",
    "        writer.writerow([test_ids[i],a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('answer.csv',\n",
    "           result,\n",
    "           delimiter=',',\n",
    "           fmt=('%d', '%d'),\n",
    "           header='Id,Response',\n",
    "           comments='',\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
