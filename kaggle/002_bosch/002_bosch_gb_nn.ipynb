{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from xgboost import XGBClassifier\n",
    "import sklearn.ensemble\n",
    "from sklearn.metrics import matthews_corrcoef, roc_auc_score\n",
    "from sklearn.cross_validation import cross_val_score, StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Selecting important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "date_chunks = pd.read_csv(\"./train_date.csv\", index_col=0, chunksize=10000, dtype=np.float32)\n",
    "num_chunks = pd.read_csv(\"./train_numeric.csv\", index_col=0,\n",
    "                         usecols=list(range(969)), chunksize=10000, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = pd.concat([pd.concat([dchunk, nchunk], axis=1).sample(frac=0.0.5) for dchunk, nchunk in zip(date_chunks, num_chunks)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_all = pd.read_csv(\"./train_numeric.csv\", index_col=0, usecols=[0,969], dtype=np.float32)\n",
    "y = y_all.loc[X.index].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.,  1.], dtype=float32), array([235391,   1358]))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.005, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0, learning_rate=0.01, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
       "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = XGBClassifier(base_score=0.005)\n",
    "clf.fit(X.values, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "important_indices = np.where(clf.feature_importances_>0.007)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 393,  440,  805,  883,  939, 1018, 1019, 1029, 1042, 1050, 1056,\n",
       "       1188, 1271, 1392, 1497, 1501, 1512, 1516, 1548, 1549, 1550, 1847,\n",
       "       1883, 1893, 1897, 1900, 1907, 1914, 1944, 1955, 1974, 1985, 1992,\n",
       "       1994, 2004, 2006, 2007, 2010, 2017, 2040])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40,)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select cols\n",
    "n_date_features = 1156\n",
    "\n",
    "date_cols = np.concatenate([[0], important_indices[important_indices < n_date_features] + 1])\n",
    "numeric_cols = np.concatenate([[0], important_indices[important_indices >= n_date_features] + 1 - n_date_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_date_features = len(important_indices[important_indices < n_date_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1272"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_date = pd.read_csv(\"./train_date.csv\", index_col=0, dtype=np.float32, usecols=date_cols)\n",
    "# X_date.fillna(0, inplace=True)\n",
    "X_num = pd.read_csv(\"./train_numeric.csv\", index_col=0, dtype=np.float32, usecols=numeric_cols)\n",
    "# X_num.fillna(0, inplace=True)\n",
    "X = pd.concat([X_date, X_num ], axis=1)\n",
    "\n",
    "del(X_date)\n",
    "del(X_num)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = pd.read_csv(\"./train_numeric.csv\", index_col=0, dtype=np.float32, usecols=[0,969]).values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normalize date by columns\n",
    "for i, c in enumerate(X.columns):\n",
    "    if i + 1 > total_date_features:\n",
    "        break\n",
    "    X[c] = X[c]/X[c].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_date_test = pd.read_csv(\"./test_date.csv\", index_col=0, dtype=np.float32, usecols=date_cols)\n",
    "# X_date_test.fillna(0, inplace=True)\n",
    "X_num_test = pd.read_csv(\"./test_numeric.csv\", index_col=0, dtype=np.float32, usecols=numeric_cols)\n",
    "# X_num_test.fillna(0, inplace=True)\n",
    "\n",
    "X_test = pd.concat([X_date_test, X_num_test ], axis=1)\n",
    "    \n",
    "del(X_date_test)\n",
    "del(X_num_test)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize date by columns\n",
    "for i, c in enumerate(X_test.columns):\n",
    "    if i + 1 > total_date_features:\n",
    "        break\n",
    "    X_test[c] = X_test[c]/X[c].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_one_hot(y, nlabels=None):\n",
    "    if nlabels == None:\n",
    "        nlabels = np.unique(y).__len__()\n",
    "    return (np.arange(nlabels) == y[:,None]).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wb(wshape=[None], bshape=[None], device='/cpu:0'):\n",
    "    with tf.device(device):\n",
    "        w = tf.get_variable(\"w\", wshape, initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        b = tf.get_variable('b', bshape, initializer=tf.constant_initializer(0.0))\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_oh = to_one_hot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_idx = np.where(y==1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data (32, 88)\n",
      "layer1 (32, 512)\n",
      "layer2 (32, 128)\n",
      "layer3 (32, 32)\n",
      "layer4 (32, 2)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "l2_reg_norm = 5e-5\n",
    "features = X.shape[1]\n",
    "train_size = X.shape[0]\n",
    "\n",
    "widest = 512\n",
    "\n",
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    X_tf = tf.placeholder(tf.float32, shape=(batch_size, features))\n",
    "    y_tf = tf.placeholder(tf.float32, shape=(batch_size,2))\n",
    "\n",
    "    with tf.variable_scope(\"Layer1\"):\n",
    "            layer1_weights, layer1_biases = wb([features, widest], [widest])\n",
    "    with tf.variable_scope(\"Layer2\"):\n",
    "            layer2_weights, layer2_biases = wb([widest, widest//4], [widest//4])\n",
    "    with tf.variable_scope(\"Layer3\"):\n",
    "            layer3_weights, layer3_biases = wb([widest//4, widest//16], [widest//16])\n",
    "    with tf.variable_scope(\"Layer4\"):\n",
    "            layer4_weights, layer4_biases = wb([widest//16, 2], [2])\n",
    "\n",
    "    def model(data, train=True):\n",
    "        print(\"data\", data.get_shape())\n",
    "        \n",
    "        layer1 = tf.nn.tanh(tf.matmul(data, layer1_weights) + layer1_biases)\n",
    "        print(\"layer1\", layer1.get_shape())\n",
    "        if train:\n",
    "            tf.nn.dropout(layer1, 0.5)\n",
    "            \n",
    "            \n",
    "        layer2 = tf.nn.tanh(tf.matmul(layer1, layer2_weights) + layer2_biases)\n",
    "        print(\"layer2\", layer2.get_shape())\n",
    "        if train:\n",
    "            tf.nn.dropout(layer2, 0.5)\n",
    "            \n",
    "        layer3 = tf.nn.tanh(tf.matmul(layer2, layer3_weights) + layer3_biases)\n",
    "        print(\"layer3\", layer3.get_shape())\n",
    "        if train:\n",
    "            tf.nn.dropout(layer3, 0.5)\n",
    "        \n",
    "        \n",
    "        layer4 = tf.nn.tanh(tf.matmul(layer3, layer4_weights) + layer4_biases)\n",
    "        print(\"layer4\", layer4.get_shape())\n",
    "\n",
    "        return layer4\n",
    "\n",
    "    logits = model(X_tf)\n",
    "\n",
    "    loss_data = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, y_tf))\n",
    "    regularizers = (tf.nn.l2_loss(layer1_weights) + tf.nn.l2_loss(layer1_biases) +\n",
    "                    tf.nn.l2_loss(layer2_weights) + tf.nn.l2_loss(layer2_biases) +\n",
    "                    tf.nn.l2_loss(layer3_weights) + tf.nn.l2_loss(layer3_biases) +\n",
    "                    tf.nn.l2_loss(layer4_weights) + tf.nn.l2_loss(layer4_biases))\n",
    "    loss_l2 = l2_reg_norm * regularizers\n",
    "    loss = loss_data + loss_l2\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    learn_rate  = tf.train.exponential_decay(.0001, global_step*batch_size, train_size//2, 0.5, staircase=True)\n",
    "    optimizer = tf.train.AdamOptimizer(learn_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    train_prediction = tf.nn.softmax(logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394580\n",
      "Initialized valiables\n",
      "0 0.708609 0.685225 0.0 62.5 0:00:00.028392\n",
      "10 0.684355 0.661016 -0.1490711985 68.75 0:00:00.032567\n",
      "20 0.664881 0.641567 0.0 75.0 0:00:00.030993\n",
      "30 0.679973 0.656679 0.0 75.0 0:00:00.030733\n",
      "40 0.620125 0.596849 0.0 75.0 0:00:00.029296\n",
      "50 0.644952 0.621692 0.0 75.0 0:00:00.031232\n",
      "60 0.615031 0.591785 0.0 75.0 0:00:00.037857\n",
      "70 0.62802 0.60479 0.0 75.0 0:00:00.038687\n",
      "80 0.584962 0.561746 0.0 75.0 0:00:00.030472\n",
      "90 0.614844 0.591645 0.0 75.0 0:00:00.033946\n",
      "10000 0.558355 0.539945 0.0 75.0 0:00:36.046797\n",
      "20000 0.518118 0.500101 0.4472135955 81.25 0:00:43.461390\n",
      "30000 0.284131 0.265621 0.654653670708 87.5 0:00:42.772601\n",
      "[[ 0.94834739  0.05165254]\n",
      " [ 0.94661367  0.0533864 ]\n",
      " [ 0.87933207  0.12066793]\n",
      " [ 0.88647085  0.1135292 ]\n",
      " [ 0.76154464  0.23845528]\n",
      " [ 0.92539084  0.07460915]\n",
      " [ 0.82630634  0.17369373]\n",
      " [ 0.94885856  0.0511414 ]\n",
      " [ 0.90955901  0.09044105]\n",
      " [ 0.82519352  0.17480646]\n",
      " [ 0.84306574  0.15693428]\n",
      " [ 0.94136161  0.05863838]\n",
      " [ 0.63443494  0.365565  ]\n",
      " [ 0.18765897  0.81234103]\n",
      " [ 0.43785548  0.56214452]\n",
      " [ 0.62897569  0.37102431]]\n",
      "40000 0.302241 0.282987 0.856348838578 93.75 0:00:45.398976\n",
      "50000 0.239161 0.219304 0.654653670708 87.5 0:00:45.464347\n",
      "60000 0.281188 0.260682 0.666666666667 87.5 0:00:45.121979\n",
      "[[ 0.81558627  0.18441378]\n",
      " [ 0.97972304  0.020277  ]\n",
      " [ 0.94897532  0.05102471]\n",
      " [ 0.58939278  0.41060722]\n",
      " [ 0.58939278  0.41060722]\n",
      " [ 0.45174447  0.54825544]\n",
      " [ 0.88365495  0.11634506]\n",
      " [ 0.84146321  0.15853682]\n",
      " [ 0.92958111  0.07041887]\n",
      " [ 0.94644499  0.05355497]\n",
      " [ 0.93034714  0.06965289]\n",
      " [ 0.940768    0.05923198]\n",
      " [ 0.35215893  0.6478411 ]\n",
      " [ 0.15201157  0.84798849]\n",
      " [ 0.56203973  0.4379603 ]\n",
      " [ 0.05823218  0.94176787]]\n",
      "70000 0.330779 0.309598 0.654653670708 87.5 0:00:44.324787\n",
      "80000 0.292138 0.270447 0.878310065654 93.75 0:00:43.943944\n",
      "90000 0.190677 0.168584 0.832050294338 93.75 0:00:47.796173\n",
      "[[ 0.9801318   0.01986821]\n",
      " [ 0.61004937  0.3899506 ]\n",
      " [ 0.99310452  0.00689543]\n",
      " [ 0.92992914  0.07007089]\n",
      " [ 0.94211298  0.05788701]\n",
      " [ 0.83206755  0.16793241]\n",
      " [ 0.95081544  0.04918464]\n",
      " [ 0.71515095  0.28484902]\n",
      " [ 0.98467237  0.01532761]\n",
      " [ 0.99257964  0.00742038]\n",
      " [ 0.98781198  0.01218801]\n",
      " [ 0.94660693  0.05339311]\n",
      " [ 0.25667092  0.74332905]\n",
      " [ 0.53856796  0.46143204]\n",
      " [ 0.17955044  0.82044959]\n",
      " [ 0.10984325  0.89015669]]\n",
      "100000 0.248507 0.226019 0.856348838578 93.75 0:00:44.755533\n",
      "110000 0.524248 0.501372 0.333333333333 75.0 0:00:43.793798\n",
      "120000 0.253285 0.230168 0.666666666667 87.5 0:00:52.285926\n",
      "[[ 0.99635196  0.00364806]\n",
      " [ 0.45202026  0.54797977]\n",
      " [ 0.99802172  0.00197827]\n",
      " [ 0.81687361  0.18312645]\n",
      " [ 0.98834831  0.01165168]\n",
      " [ 0.9867754   0.01322463]\n",
      " [ 0.93672067  0.0632793 ]\n",
      " [ 0.81502885  0.18497109]\n",
      " [ 0.81502885  0.18497109]\n",
      " [ 0.94254202  0.05745802]\n",
      " [ 0.97695941  0.02304064]\n",
      " [ 0.86720586  0.13279413]\n",
      " [ 0.80099505  0.19900501]\n",
      " [ 0.09411098  0.90588909]\n",
      " [ 0.10734156  0.89265847]\n",
      " [ 0.12147106  0.87852895]]\n",
      "130000 0.352764 0.329423 0.7453559925 87.5 0:00:51.783004\n",
      "140000 0.327671 0.30412 0.462250163521 81.25 0:00:50.433942\n",
      "150000 0.106992 0.0832511 1.0 100.0 0:00:50.712000\n",
      "[[ 0.81831861  0.18168135]\n",
      " [ 0.96316284  0.03683713]\n",
      " [ 0.97032309  0.02967692]\n",
      " [ 0.99042553  0.00957444]\n",
      " [ 0.9694131   0.03058693]\n",
      " [ 0.90960693  0.09039304]\n",
      " [ 0.87736988  0.12263008]\n",
      " [ 0.87496758  0.12503244]\n",
      " [ 0.98702687  0.0129732 ]\n",
      " [ 0.99014437  0.00985562]\n",
      " [ 0.97124183  0.02875818]\n",
      " [ 0.97793293  0.02206704]\n",
      " [ 0.04079245  0.95920759]\n",
      " [ 0.03729158  0.96270841]\n",
      " [ 0.22480966  0.77519035]\n",
      " [ 0.2252976   0.77470243]]\n",
      "160000 0.149398 0.125548 0.832050294338 93.75 0:00:58.495512\n",
      "170000 0.139984 0.116023 1.0 100.0 0:00:57.780890\n",
      "180000 0.237697 0.213631 0.462250163521 81.25 0:00:53.673069\n",
      "[[  4.61584538e-01   5.38415432e-01]\n",
      " [  9.96325076e-01   3.67492624e-03]\n",
      " [  9.25055146e-01   7.49448612e-02]\n",
      " [  9.95787442e-01   4.21257876e-03]\n",
      " [  9.88949835e-01   1.10501647e-02]\n",
      " [  9.65723455e-01   3.42765637e-02]\n",
      " [  9.50780630e-01   4.92194034e-02]\n",
      " [  9.99982953e-01   1.70547301e-05]\n",
      " [  9.99982953e-01   1.70547301e-05]\n",
      " [  9.99962449e-01   3.75547352e-05]\n",
      " [  9.93544340e-01   6.45565195e-03]\n",
      " [  9.74101663e-01   2.58983821e-02]\n",
      " [  2.83981324e-03   9.97160196e-01]\n",
      " [  7.16043353e-01   2.83956736e-01]\n",
      " [  6.67591929e-01   3.32408130e-01]\n",
      " [  6.46076277e-02   9.35392380e-01]]\n",
      "190000 0.112114 0.0879672 1.0 100.0 0:00:49.836152\n",
      "200000 0.221606 0.197406 0.856348838578 93.75 0:00:56.868694\n",
      "210000 0.157776 0.133521 1.0 100.0 0:00:57.122820\n",
      "[[  9.95737314e-01   4.26268298e-03]\n",
      " [  9.96627271e-01   3.37267830e-03]\n",
      " [  9.18271124e-01   8.17288607e-02]\n",
      " [  9.99175251e-01   8.24758317e-04]\n",
      " [  8.51710558e-01   1.48289427e-01]\n",
      " [  9.79857385e-01   2.01426111e-02]\n",
      " [  9.04712379e-01   9.52876434e-02]\n",
      " [  9.90967453e-01   9.03252885e-03]\n",
      " [  6.78910851e-01   3.21089149e-01]\n",
      " [  9.43325818e-01   5.66741116e-02]\n",
      " [  9.86693084e-01   1.33068720e-02]\n",
      " [  8.01358104e-01   1.98641881e-01]\n",
      " [  1.95644587e-01   8.04355383e-01]\n",
      " [  2.71344453e-01   7.28655577e-01]\n",
      " [  4.48838361e-02   9.55116212e-01]\n",
      " [  3.88523519e-01   6.11476541e-01]]\n",
      "220000 0.505531 0.481221 0.462250163521 81.25 0:00:48.128651\n",
      "230000 0.157044 0.1327 0.832050294338 93.75 0:00:44.023194\n",
      "240000 0.162898 0.138526 1.0 100.0 0:00:44.799742\n",
      "[[ 0.98484772  0.01515227]\n",
      " [ 0.93139672  0.06860321]\n",
      " [ 0.88455969  0.11544028]\n",
      " [ 0.99833846  0.00166155]\n",
      " [ 0.9773252   0.02267485]\n",
      " [ 0.93792814  0.06207182]\n",
      " [ 0.97561502  0.02438497]\n",
      " [ 0.91422969  0.08577028]\n",
      " [ 0.95259643  0.04740359]\n",
      " [ 0.99881881  0.00118111]\n",
      " [ 0.99413449  0.00586558]\n",
      " [ 0.97488111  0.02511896]\n",
      " [ 0.49064302  0.50935704]\n",
      " [ 0.40214595  0.59785402]\n",
      " [ 0.13707991  0.86292005]\n",
      " [ 0.32080793  0.67919213]]\n",
      "250000 0.265282 0.240882 0.712525303194 87.5 0:00:50.085050\n",
      "260000 0.118435 0.0940098 1.0 100.0 0:00:46.015725\n",
      "270000 0.250944 0.226504 1.0 100.0 0:00:50.167251\n",
      "[[ 0.97474968  0.02525035]\n",
      " [ 0.88248301  0.11751706]\n",
      " [ 0.99856561  0.0014344 ]\n",
      " [ 0.99739885  0.00260112]\n",
      " [ 0.9973526   0.00264738]\n",
      " [ 0.56299657  0.43700349]\n",
      " [ 0.51494259  0.48505744]\n",
      " [ 0.91654372  0.08345621]\n",
      " [ 0.72532409  0.27467588]\n",
      " [ 0.9611029   0.03889708]\n",
      " [ 0.68068391  0.31931612]\n",
      " [ 0.99572539  0.00427456]\n",
      " [ 0.03534095  0.96465909]\n",
      " [ 0.48338613  0.51661384]\n",
      " [ 0.29364061  0.70635939]\n",
      " [ 0.29364061  0.70635939]]\n",
      "280000 0.235489 0.211036 0.856348838578 93.75 0:00:58.022728\n",
      "290000 0.290978 0.266511 0.654653670708 87.5 0:00:43.353185\n",
      "300000 0.121723 0.0972457 1.0 100.0 0:00:45.320539\n",
      "[[ 0.99741387  0.0025861 ]\n",
      " [ 0.76055974  0.23944025]\n",
      " [ 0.98707342  0.01292658]\n",
      " [ 0.99648166  0.00351833]\n",
      " [ 0.95255405  0.04744596]\n",
      " [ 0.94765824  0.05234173]\n",
      " [ 0.99840599  0.00159408]\n",
      " [ 0.99554271  0.00445735]\n",
      " [ 0.78466392  0.21533605]\n",
      " [ 0.53556859  0.46443141]\n",
      " [ 0.99639052  0.0036094 ]\n",
      " [ 0.83921593  0.16078405]\n",
      " [ 0.03940588  0.96059412]\n",
      " [ 0.03164143  0.96835864]\n",
      " [ 0.03164143  0.96835864]\n",
      " [ 0.00432628  0.99567372]]\n",
      "310000 0.132507 0.108023 1.0 100.0 0:00:46.194600\n",
      "320000 0.114577 0.0900852 1.0 100.0 0:00:48.520893\n",
      "330000 0.653115 0.628616 0.333333333333 75.0 0:00:45.463938\n",
      "[[ 0.99760592  0.0023941 ]\n",
      " [ 0.9801206   0.01987948]\n",
      " [ 0.02481432  0.97518569]\n",
      " [ 0.99754339  0.00245656]\n",
      " [ 0.99336374  0.00663631]\n",
      " [ 0.98383522  0.0161648 ]\n",
      " [ 0.90926647  0.09073348]\n",
      " [ 0.9745059   0.02549416]\n",
      " [ 0.969872    0.03012807]\n",
      " [ 0.99720639  0.0027936 ]\n",
      " [ 0.97057009  0.02942998]\n",
      " [ 0.24552897  0.754471  ]\n",
      " [ 0.96357697  0.036423  ]\n",
      " [ 0.00838202  0.99161798]\n",
      " [ 0.16744117  0.83255881]\n",
      " [ 0.70502561  0.29497436]]\n",
      "340000 0.209262 0.184759 0.666666666667 87.5 0:00:46.634245\n",
      "350000 0.189367 0.16486 1.0 100.0 0:00:46.578713\n",
      "360000 0.309467 0.284956 0.462250163521 81.25 0:00:50.089077\n",
      "[[ 0.90942895  0.09057105]\n",
      " [ 0.3796165   0.62038356]\n",
      " [ 0.76944929  0.23055066]\n",
      " [ 0.86877614  0.13122387]\n",
      " [ 0.98831451  0.01168555]\n",
      " [ 0.99785632  0.00214368]\n",
      " [ 0.99553764  0.00446239]\n",
      " [ 0.972628    0.02737199]\n",
      " [ 0.9655776   0.03442236]\n",
      " [ 0.98220557  0.0177945 ]\n",
      " [ 0.99752575  0.00247428]\n",
      " [ 0.99424052  0.00575953]\n",
      " [ 0.61288184  0.38711816]\n",
      " [ 0.01560871  0.98439121]\n",
      " [ 0.45126104  0.54873896]\n",
      " [ 0.75847089  0.24152906]]\n",
      "370000 0.136765 0.112252 1.0 100.0 0:00:51.809122\n",
      "380000 0.174061 0.149545 1.0 100.0 0:00:46.185193\n",
      "390000 0.164397 0.13988 1.0 100.0 0:00:49.257204\n",
      "[[ 0.77423608  0.22576392]\n",
      " [ 0.99827754  0.0017225 ]\n",
      " [ 0.88700628  0.11299378]\n",
      " [ 0.94924164  0.05075831]\n",
      " [ 0.58879304  0.41120696]\n",
      " [ 0.99538875  0.00461126]\n",
      " [ 0.96837616  0.03162382]\n",
      " [ 0.99286675  0.00713327]\n",
      " [ 0.99716502  0.00283501]\n",
      " [ 0.80122989  0.19877003]\n",
      " [ 0.99694294  0.00305706]\n",
      " [ 0.98730612  0.01269391]\n",
      " [ 0.31132448  0.68867552]\n",
      " [ 0.05891667  0.94108337]\n",
      " [ 0.38650116  0.61349875]\n",
      " [ 0.06975939  0.93024057]]\n"
     ]
    }
   ],
   "source": [
    "pos_size = 4\n",
    "batch_size_part = batch_size-pos_size\n",
    "num_steps = train_size//batch_size_part * 4\n",
    "\n",
    "print(num_steps)\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init_op = tf.initialize_all_variables()\n",
    "    saver = tf.train.Saver()\n",
    "    init_op.run()\n",
    "    print(\"Initialized valiables\")\n",
    "    t = datetime.datetime.now()\n",
    "    for i in range(num_steps):\n",
    "        offset = (i*batch_size_part) % (X.shape[0] - batch_size_part)\n",
    "        X_ = X.values[offset:offset+batch_size_part]\n",
    "        y_ = y_oh[offset:offset+batch_size_part]\n",
    "        \n",
    "        offset_pos = (i*pos_size) % (len(pos_idx) - pos_size)\n",
    "        pos_idx_ = pos_idx[offset_pos:offset_pos+pos_size]\n",
    "        y__ = to_one_hot(np.array([1]*pos_size), 2)\n",
    "        X__ = X.iloc[pos_idx_]\n",
    "        \n",
    "        X_ = np.concatenate((X_, X__), axis=0)\n",
    "        y_ = np.concatenate((y_, y__), axis=0)\n",
    "                                   \n",
    "        feed_dict = {X_tf : X_, y_tf : y_}\n",
    "        _, l, ld, pred = sess.run([optimizer, loss, loss_data, train_prediction], feed_dict=feed_dict)\n",
    "        if (i < 100 and i%10 == 0) or i%10000 == 0:\n",
    "            \n",
    "            print(i, l, ld, matthews_corrcoef(np.argmax(y_, 1), np.argmax(pred, 1) ), accuracy(pred, y_), datetime.datetime.now() - t)\n",
    "            t = datetime.datetime.now()\n",
    "        if i>0 and i%30000 == 0:\n",
    "            print(pred)\n",
    "            save_path = saver.save(sess, \"bosch_sel_{}.ckpt\".format(i))\n",
    "    save_path = saver.save(sess, \"bosch_sel_end.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1183748, 50)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data (?, 88)\n",
      "layer1 (?, 256)\n",
      "layer2 (?, 256)\n",
      "layer3 (?, 128)\n",
      "layer4 (?, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ans = np.array([])\n",
    "with tf.Session(graph = graph) as sess:\n",
    "    init_op = tf.initialize_all_variables()\n",
    "    saver = tf.train.Saver()\n",
    "    init_op.run()\n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, \"bosch_sel_end.ckpt\")\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        X_tf_test = tf.placeholder(tf.float32, shape=(None, features))\n",
    "        test_prediction = tf.nn.softmax(model(X_tf_test, train=False))\n",
    "    \n",
    "    ans = sess.run(test_prediction, feed_dict = {X_tf_test:X_test.values})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ans = np.argmax(ans, axis=1)\n",
    "# 88x128x128x128x2 / .0001 / x4 - 0.00483\n",
    "# 88x256x256x128x2 / .0001 / x4 - 0.00635\n",
    "# 50x512x256x128x2 / .0001 / x4 - 0.09744 - clf.feature_importances_>0.007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"sample_submission.csv\")\n",
    "submission.iloc[:, 1] = ans\n",
    "submission.to_csv('answer.gb.nn.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0, ROC AUC: 0.493\n",
      "fold 1, ROC AUC: 0.499\n",
      "fold 2, ROC AUC: 0.502\n",
      "0.497108472527\n"
     ]
    }
   ],
   "source": [
    "clf = XGBClassifier(max_depth=5, base_score=0.005)\n",
    "cv = StratifiedKFold(y, n_folds=3)\n",
    "preds = np.ones(y.shape[0])\n",
    "for i, (train, test) in enumerate(cv):\n",
    "    preds[test] = clf.fit(X.values[train], y[train]).predict_proba(X.values[test])[:,1]\n",
    "    print(\"fold {}, ROC AUC: {:.3f}\".format(i, roc_auc_score(y[test], preds[test])))\n",
    "print(roc_auc_score(y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000258103875806\n"
     ]
    }
   ],
   "source": [
    "# pick the best threshold out-of-fold\n",
    "thresholds = np.linspace(0.01, 0.99, 50)\n",
    "mcc = np.array([matthews_corrcoef(y, preds>thr) for thr in thresholds])\n",
    "best_threshold = thresholds[mcc.argmax()]\n",
    "print(mcc.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ans = (clf.predict_proba(X_test.values)[:,1] > best_threshold).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1183748,)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Result - 0.20991\n",
    "submission = pd.read_csv(\"sample_submission.csv\")\n",
    "submission.iloc[:, 1] = ans\n",
    "submission.to_csv('answer.gb.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
