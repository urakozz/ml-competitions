{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.ensemble\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainfiles = ['train_categorical.csv',\n",
    "                  'train_date.csv',\n",
    "                  'train_numeric.csv']\n",
    "testfiles = ['test_categorical.csv',\n",
    "                 'test_date.csv',\n",
    "                 'test_numeric.csv']\n",
    "\n",
    "cols = [['Id',\n",
    "             'L1_S24_F1559', 'L3_S32_F3851',\n",
    "             'L1_S24_F1827', 'L1_S24_F1582',\n",
    "             'L3_S32_F3854', 'L1_S24_F1510',\n",
    "             'L1_S24_F1525'],\n",
    "            ['Id',\n",
    "             'L3_S30_D3496', 'L3_S30_D3506',\n",
    "             'L3_S30_D3501', 'L3_S30_D3516',\n",
    "             'L3_S30_D3511'],\n",
    "            ['Id',\n",
    "             'L1_S24_F1846', 'L3_S32_F3850',\n",
    "             'L1_S24_F1695', 'L1_S24_F1632',\n",
    "             'L3_S33_F3855', 'L1_S24_F1604',\n",
    "             'L3_S29_F3407', 'L3_S33_F3865',\n",
    "             'L3_S38_F3952', 'L1_S24_F1723',\n",
    "             'Response']]\n",
    "cat_features = 94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_categorical.csv\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "train_date.csv\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "train_numeric.csv\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "traindata = None\n",
    "for i, f in enumerate(trainfiles):\n",
    "    print(f)\n",
    "    subset = None\n",
    "    for i, chunk in enumerate(pd.read_csv(f, usecols=cols[i], chunksize=50000, low_memory=False)):\n",
    "        print(i)\n",
    "        if subset is None:\n",
    "            subset = chunk.copy()\n",
    "        else:\n",
    "            subset = pd.concat([subset, chunk])\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    if traindata is None:\n",
    "        traindata = subset.copy()\n",
    "    else:\n",
    "        traindata = pd.merge(traindata, subset.copy(), on=\"Id\")\n",
    "    del subset\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_categorical.csv\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "test_date.csv\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "test_numeric.csv\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "testdata = None\n",
    "del cols[2][-1]\n",
    "for i, f in enumerate(testfiles):\n",
    "    print(f)\n",
    "    subset = None\n",
    "    for i, chunk in enumerate(pd.read_csv(f, usecols=cols[i], chunksize=25000, low_memory=False)):\n",
    "        print(i)\n",
    "        if subset is None:\n",
    "            subset = chunk.copy()\n",
    "        else:\n",
    "            subset = pd.concat([subset, chunk])\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    if testdata is None:\n",
    "        testdata = subset.copy()\n",
    "    else:\n",
    "        testdata = pd.merge(testdata, subset.copy(), on=\"Id\")\n",
    "    del subset\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def LeaveOneOut(data1, data2, columnName, useLOO=False):\n",
    "    grpOutcomes = data1.groupby(columnName)['Response'].mean().reset_index()\n",
    "    grpCount = data1.groupby(columnName)['Response'].count().reset_index()\n",
    "    grpOutcomes['cnt'] = grpCount.Response\n",
    "    if(useLOO):\n",
    "        grpOutcomes = grpOutcomes[grpOutcomes.cnt > 1]\n",
    "    grpOutcomes.drop('cnt', inplace=True, axis=1)\n",
    "    outcomes = data2['Response'].values\n",
    "    x = pd.merge(data2[[columnName, 'Response']], grpOutcomes,\n",
    "                 suffixes=('x_', ''),\n",
    "                 how='left',\n",
    "                 on=columnName,\n",
    "                 left_index=True)['Response']\n",
    "    if(useLOO):\n",
    "        x = ((x*x.shape[0])-outcomes)/(x.shape[0]-1)\n",
    "        #  x = x + np.random.normal(0, .01, x.shape[0])\n",
    "    return x.fillna(x.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Id', 'L1_S24_F1510', 'L1_S24_F1525', 'L1_S24_F1559', 'L1_S24_F1582',\n",
      "       'L1_S24_F1827', 'L3_S32_F3851', 'L3_S32_F3854', 'L3_S30_D3496',\n",
      "       'L3_S30_D3501', 'L3_S30_D3506', 'L3_S30_D3511', 'L3_S30_D3516',\n",
      "       'L1_S24_F1604', 'L1_S24_F1632', 'L1_S24_F1695', 'L1_S24_F1723',\n",
      "       'L1_S24_F1846', 'L3_S29_F3407', 'L3_S32_F3850', 'L3_S33_F3855',\n",
      "       'L3_S33_F3865', 'L3_S38_F3952', 'Response'],\n",
      "      dtype='object')\n",
      "L1_S24_F1559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yury/anaconda/envs/tensorflow/lib/python3.5/site-packages/pandas/core/indexing.py:465: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L3_S32_F3851\n",
      "L1_S24_F1827\n",
      "L1_S24_F1582\n",
      "L3_S32_F3854\n",
      "L1_S24_F1510\n",
      "L1_S24_F1525\n",
      "L3_S30_D3496\n",
      "L3_S30_D3506\n",
      "L3_S30_D3501\n",
      "L3_S30_D3516\n",
      "L3_S30_D3511\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata['Response'] = 0\n",
    "visibletraindata = traindata[::2]\n",
    "blindtraindata = traindata[1::2]\n",
    "print(blindtraindata.columns)\n",
    "for i in range(2):\n",
    "    for col in cols[i][1:]:\n",
    "        print(col)\n",
    "        blindtraindata.loc[:, col] = LeaveOneOut(visibletraindata,\n",
    "                                                 blindtraindata,\n",
    "                                                 col, False).values\n",
    "        testdata.loc[:, col] = LeaveOneOut(visibletraindata,\n",
    "                                           testdata, col, False).values\n",
    "del visibletraindata\n",
    "\n",
    "testdata.drop('Response', inplace=True, axis=1)\n",
    "traindata = blindtraindata\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "traindata.fillna(0, inplace=True)\n",
    "testdata.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = sklearn.ensemble.GradientBoostingClassifier(n_estimators=10000, max_depth=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.0654          123.65m\n",
      "         2           0.0646          133.05m\n",
      "         3           0.0643          137.07m\n",
      "         4           0.1072          138.23m\n",
      "         5           0.1068          137.94m\n",
      "         6           0.1066          138.74m\n",
      "         7           0.1064          136.21m\n",
      "         8           0.1062          137.65m\n",
      "         9           0.1059          136.98m\n",
      "        10           0.1058          135.93m\n",
      "        20           0.0927          126.20m\n",
      "        30           0.0923          120.02m\n",
      "        40           0.0920          116.84m\n",
      "        50           0.0918          116.61m\n",
      "        60           0.0917          114.96m\n",
      "        70           0.0916          115.10m\n",
      "        80           0.0915          115.26m\n",
      "        90           0.0914          114.44m\n",
      "       100           0.0913          114.83m\n",
      "       200           0.0904          111.13m\n",
      "       300           0.0896          107.83m\n",
      "       400           0.0889          105.58m\n",
      "       500           0.0881          102.83m\n",
      "       600           0.0877          100.63m\n",
      "       700           0.0870           99.06m\n",
      "       800           0.0867           97.58m\n",
      "       900           0.0863           96.31m\n",
      "      1000           0.0860           95.15m\n",
      "      2000           0.0830           83.72m\n",
      "      3000           0.0812           73.28m\n",
      "      4000           0.0798           62.58m\n",
      "      5000           0.0786           52.16m\n",
      "      6000           0.0777           41.73m\n",
      "      7000           0.0769           31.31m\n",
      "      8000           0.0763           20.90m\n",
      "      9000           0.0758           10.43m\n",
      "     10000           0.0753            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
       "              max_depth=5, max_features=None, max_leaf_nodes=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=10000,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=1,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "clf.fit(traindata.values[:, 1:-1], traindata.values[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# num_rounds = 50\n",
    "\n",
    "# params = {}\n",
    "# params['objective'] = \"binary:logistic\"\n",
    "# params['eta'] = 0.021\n",
    "# params['max_depth'] = 7\n",
    "# params['colsample_bytree'] = 0.82\n",
    "# params['min_child_weight'] = 3\n",
    "# params['base_score'] = 0.005\n",
    "# params['silent'] = False\n",
    "\n",
    "# features = traindata.columns[1:-1]\n",
    "\n",
    "# dtrain = xgb.DMatrix(traindata[features],\n",
    "#                     train.Response,\n",
    "#                     silent=True)\n",
    "# dtest = xgb.DMatrix(testdata[features],\n",
    "#                     silent=True)\n",
    "\n",
    "# watchlist = [(dtrain, 'train'), (dtrain, 'val')]\n",
    "# clf = xgb.train(params, dtrain,\n",
    "#                         num_boost_round=num_rounds,\n",
    "#                         evals=watchlist,\n",
    "#                         early_stopping_rounds=20,\n",
    "#                         maximize=True\n",
    "#                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ans = clf.predict(testdata.values[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ans = ans.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"sample_submission.csv\")\n",
    "submission.iloc[:, 1] = ans\n",
    "submission.to_csv('answer.gb.csv', index=None)\n",
    "# 0.20332"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
