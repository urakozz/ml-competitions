{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_is = np.genfromtxt ('train_numeric.csv', delimiter=\",\", skip_header=2001, usecols = (0,), dtype=int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = pd.read_csv('train_numeric.csv', index_col=0, nrows=1, low_memory=False)\n",
    "cols_d = dict()\n",
    "for i, c in enumerate(cols.columns):\n",
    "    cols_d[c] = i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_len = len(train_is)\n",
    "cat_features = 94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 969), (2000, 2140))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = pd.read_csv('train_numeric.csv', index_col=0, skiprows=1, header=None, nrows=2000, dtype=np.float32)\n",
    "num.fillna(0., inplace=True)\n",
    "cat = pd.read_csv('train_categorical_c.csv', index_col=0, skiprows=1, header=None, nrows=2000, dtype=np.float32)\n",
    "cat = cat.div(cat_features,axis='columns')\n",
    "(num.shape, cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# num.groupby(cols_d['L1_S24_F1846'])[cols_d['Response']].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 968), (2000,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = pd.concat([cat, num], axis=1)\n",
    "data = num\n",
    "X_valid = data.values[:,:-1]\n",
    "y_valid = data.values[:,-1]\n",
    "(X_valid.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "data_num_chunks = pd.read_csv('train_numeric.csv'      , index_col=0, chunksize=1000, skiprows=2001, dtype=np.float32, header=None)\n",
    "data_cat_chunks = pd.read_csv('train_categorical_c.csv', index_col=0, chunksize=1000, skiprows=2001, dtype=np.float32, header=None)\n",
    "\n",
    "X_pos = []\n",
    "y_pos = []\n",
    "i = -1\n",
    "for rows_n in data_num_chunks:\n",
    "    i+=1\n",
    "    rows_n.fillna(0., inplace=True)\n",
    "    \n",
    "#     rows_c = data_cat_chunks.__next__()\n",
    "#     rows_c = rows_c.div(cat_features, axis='columns')\n",
    "    \n",
    "#     rows = pd.concat([rows_c, rows_n], axis=1)\n",
    "    rows = rows_n\n",
    "    for row in rows.values:\n",
    "        if row[-1] == 0:\n",
    "            continue\n",
    "        X_pos.append(row[:-1])\n",
    "        y_pos.append(row[-1])\n",
    "    if i %100 == 0:\n",
    "        print(i)\n",
    "\n",
    "X_pos = np.array(X_pos).astype(np.float32)\n",
    "y_pos = np.array(y_pos).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6872, 968), (6872,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_pos.shape, y_pos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"valid_pos.pickle\", 'wb') as f:\n",
    "    data = {\n",
    "        \"X_valid\":X_valid,\n",
    "        \"y_valid\":y_valid,\n",
    "        \"X_pos\": X_pos,\n",
    "        \"y_pos\":y_pos \n",
    "    }\n",
    "    pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pickle.load(open(\"valid_pos.pickle\", 'rb'))\n",
    "X_valid = data[\"X_valid\"]\n",
    "y_valid = data[\"y_valid\"]\n",
    "X_pos = data[\"X_pos\"]\n",
    "y_pos = data[\"y_pos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 968), (2000,), (6872, 968), (6872,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_valid.shape, y_valid.shape, X_pos.shape, y_pos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def wb(wshape=[None], bshape=[None], device='/cpu:0'):\n",
    "    with tf.device(device):\n",
    "        w = tf.get_variable(\"w\", wshape, initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        b = tf.get_variable('b', bshape, initializer=tf.constant_initializer(0.0))\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def to_one_hot(y, nlabels=None):\n",
    "    if nlabels == None:\n",
    "        nlabels = np.unique(y).__len__()\n",
    "    return (np.arange(nlabels) == y[:,None]).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 2), (6872, 2))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid_oh = to_one_hot(y_valid, nlabels=2)\n",
    "y_pos_oh = to_one_hot(y_pos, nlabels=2)\n",
    "\n",
    "(y_valid_oh.shape, y_pos_oh.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#idx_pos = np.where(y == 1)[0]\n",
    "# X_test = data_num.iloc[:,1:-1].values[idx_pos]\n",
    "# y_test = y_oh[idx_pos]\n",
    "\n",
    "# idx_neg = np.where(y == 0)[0]\n",
    "# mask = np.random.choice([False, True], len(idx_neg), p=[0.999, 0.001])\n",
    "# idx_neg = idx_neg[mask]\n",
    "# print(idx_neg.shape)\n",
    "\n",
    "# X_neg = data_num.iloc[:,1:-1].values[idx_neg]\n",
    "# y_neg = y_oh[(idx_neg,)]\n",
    "# X_test = np.concatenate((X_test, X_neg), axis=0)\n",
    "# y_test = np.concatenate((y_test, y_neg), axis=0)\n",
    "# print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data (16, 968)\n",
      "layer1 (16, 1024)\n",
      "layer2 (16, 512)\n",
      "layer3 (16, 512)\n",
      "layer6 (16, 2)\n",
      "data (2000, 968)\n",
      "layer1 (2000, 1024)\n",
      "layer2 (2000, 512)\n",
      "layer3 (2000, 512)\n",
      "layer6 (2000, 2)\n",
      "data (6872, 968)\n",
      "layer1 (6872, 1024)\n",
      "layer2 (6872, 512)\n",
      "layer3 (6872, 512)\n",
      "layer6 (6872, 2)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "l2_reg_norm = 5e-5\n",
    "features = X_valid.shape[1]\n",
    "train_size = train_len\n",
    "\n",
    "widest = 1024\n",
    "\n",
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    X_tf = tf.placeholder(tf.float32, shape=(batch_size, features))\n",
    "    y_tf = tf.placeholder(tf.float32, shape=(batch_size,2))\n",
    "    X_valid_tf = tf.constant(X_valid)\n",
    "    X_pos_tf = tf.constant(X_pos)\n",
    "\n",
    "    with tf.variable_scope(\"Layer1\"):\n",
    "            layer1_weights, layer1_biases = wb([features, widest], [widest])\n",
    "    with tf.variable_scope(\"Layer2\"):\n",
    "            layer2_weights, layer2_biases = wb([widest, widest//2], [widest//2])\n",
    "    with tf.variable_scope(\"Layer3\"):\n",
    "            layer3_weights, layer3_biases = wb([widest//2, widest//2], [widest//2])\n",
    "#     with tf.variable_scope(\"Layer4\"):\n",
    "#             layer4_weights, layer4_biases = wb([widest//2, widest//4], [widest//4])\n",
    "#     with tf.variable_scope(\"Layer5\"):\n",
    "#             layer5_weights, layer5_biases = wb([widest//4,64], [64])\n",
    "    with tf.variable_scope(\"Layer6\"):\n",
    "            layer6_weights, layer6_biases = wb([widest//2, 2], [2])\n",
    "\n",
    "    def model(data, train=True):\n",
    "        print(\"data\", data.get_shape())\n",
    "        \n",
    "        layer1 = tf.nn.relu(tf.matmul(data, layer1_weights) + layer1_biases)\n",
    "        print(\"layer1\", layer1.get_shape())\n",
    " \n",
    "            \n",
    "        layer2 = tf.nn.relu(tf.matmul(layer1, layer2_weights) + layer2_biases)\n",
    "        print(\"layer2\", layer2.get_shape())\n",
    "#         if train:\n",
    "#             tf.nn.dropout(layer2, 0.5)\n",
    "            \n",
    "        layer3 = tf.nn.relu(tf.matmul(layer2, layer3_weights) + layer3_biases)\n",
    "        print(\"layer3\", layer3.get_shape())\n",
    "        if train:\n",
    "            tf.nn.dropout(layer3, 0.5)\n",
    "            \n",
    "#         layer4 = tf.nn.relu(tf.matmul(layer3, layer4_weights) + layer4_biases)\n",
    "#         print(\"layer4\", layer4.get_shape())\n",
    "#         if train:\n",
    "#             tf.nn.dropout(layer4, 0.5)\n",
    "            \n",
    "#         layer5 = tf.nn.relu(tf.matmul(layer4, layer5_weights) + layer5_biases)\n",
    "#         print(\"layer5\", layer5.get_shape())\n",
    "        \n",
    "        \n",
    "        layer6 = tf.nn.relu(tf.matmul(layer3, layer6_weights) + layer6_biases)\n",
    "        print(\"layer6\", layer6.get_shape())\n",
    "\n",
    "        return layer6\n",
    "\n",
    "    logits = model(X_tf)\n",
    "\n",
    "    loss_data = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, y_tf))\n",
    "    regularizers = (tf.nn.l2_loss(layer1_weights) + tf.nn.l2_loss(layer1_biases) +\n",
    "                    tf.nn.l2_loss(layer2_weights) + tf.nn.l2_loss(layer2_biases) +\n",
    "                    tf.nn.l2_loss(layer3_weights) + tf.nn.l2_loss(layer3_biases) +\n",
    "#                     tf.nn.l2_loss(layer4_weights) + tf.nn.l2_loss(layer4_biases) + \n",
    "#                     tf.nn.l2_loss(layer5_weights) + tf.nn.l2_loss(layer5_biases) + \n",
    "                    tf.nn.l2_loss(layer6_weights) + tf.nn.l2_loss(layer6_biases))\n",
    "    loss_l2 = l2_reg_norm * regularizers\n",
    "    loss = loss_data + loss_l2\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    learn_rate  = tf.train.exponential_decay(.0001, global_step*batch_size, train_size//4, 0.5, staircase=True)\n",
    "#     tf.scalar_summary('learning_rate', learn_rate)\n",
    "    optimizer = tf.train.AdamOptimizer(learn_rate).minimize(loss, global_step=global_step, name=\"Optimizer\")\n",
    "\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    test_prediction = tf.nn.softmax(model(X_valid_tf, train=False))\n",
    "    pos_prediction = tf.nn.softmax(model(X_pos_tf, train=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98478\n",
      "Initialized valiables\n",
      "0 1.02986 68.75\n",
      "10 0.939223 75.0\n",
      "20 0.999563 75.0\n",
      "30 0.975819 75.0\n",
      "40 1.03992 62.5\n",
      "50 0.925078 75.0\n",
      "60 0.780512 75.0\n",
      "70 0.877441 75.0\n",
      "80 0.875921 75.0\n",
      "90 0.819999 75.0\n",
      "1000 0.708791 75.0\n",
      "2000 0.856888 68.75\n",
      "3000 0.676673 75.0\n",
      "4000 0.727681 75.0\n",
      "5000 0.686212 75.0\n",
      "6000 0.763242 75.0\n",
      "7000 0.705295 75.0\n",
      "8000 0.817997 75.0\n",
      "9000 0.632697 75.0\n",
      "10000 0.711594 75.0\n",
      "10000 test 99.6\n",
      "10000 pos 0.0145518044237\n",
      "[[ 0.8857432   0.11425681  1.          0.        ]\n",
      " [ 0.82407445  0.1759256   1.          0.        ]\n",
      " [ 0.82715619  0.17284377  0.          1.        ]\n",
      " [ 0.5         0.5         0.          1.        ]\n",
      " [ 0.5         0.5         1.          0.        ]\n",
      " [ 0.5         0.5         0.          1.        ]\n",
      " [ 0.84192711  0.15807295  1.          0.        ]\n",
      " [ 0.82407445  0.1759256   1.          0.        ]\n",
      " [ 0.87333012  0.12666991  1.          0.        ]\n",
      " [ 0.92274213  0.07725783  1.          0.        ]\n",
      " [ 0.92637938  0.0736206   1.          0.        ]\n",
      " [ 0.78741443  0.21258557  1.          0.        ]\n",
      " [ 0.91833097  0.08166905  0.          1.        ]\n",
      " [ 0.79047221  0.20952785  1.          0.        ]\n",
      " [ 0.98434454  0.01565548  1.          0.        ]\n",
      " [ 0.8670119   0.13298813  1.          0.        ]]\n",
      "11000 0.708523 75.0\n",
      "12000 0.530943 75.0\n",
      "13000 0.45342 75.0\n",
      "14000 0.536914 75.0\n",
      "15000 0.537654 75.0\n",
      "16000 0.492128 75.0\n",
      "17000 0.55492 75.0\n",
      "18000 0.547883 75.0\n",
      "19000 0.547054 75.0\n",
      "20000 0.325135 100.0\n",
      "20000 test 95.05\n",
      "20000 pos 84.167636787\n",
      "[[ 0.93704033  0.06295962  1.          0.        ]\n",
      " [ 0.91593081  0.08406921  1.          0.        ]\n",
      " [ 0.95823133  0.04176867  1.          0.        ]\n",
      " [ 0.99594986  0.00405017  1.          0.        ]\n",
      " [ 0.98482597  0.015174    1.          0.        ]\n",
      " [ 0.07100581  0.92899424  0.          1.        ]\n",
      " [ 0.9626627   0.03733733  1.          0.        ]\n",
      " [ 0.99021101  0.00978901  1.          0.        ]\n",
      " [ 0.89910239  0.10089768  1.          0.        ]\n",
      " [ 0.87796742  0.12203266  1.          0.        ]\n",
      " [ 0.47800997  0.52199006  0.          1.        ]\n",
      " [ 0.09091857  0.90908146  0.          1.        ]\n",
      " [ 0.36810109  0.63189888  0.          1.        ]\n",
      " [ 0.84486991  0.15513003  1.          0.        ]\n",
      " [ 0.99390489  0.00609508  1.          0.        ]\n",
      " [ 0.91734791  0.08265211  1.          0.        ]]\n",
      "21000 0.243565 100.0\n",
      "22000 0.272734 100.0\n",
      "23000 0.328232 93.75\n",
      "24000 0.243429 100.0\n",
      "25000 0.243905 100.0\n",
      "26000 0.303542 100.0\n",
      "27000 0.274201 93.75\n",
      "28000 0.331969 93.75\n",
      "29000 0.270433 100.0\n",
      "30000 0.396756 93.75\n",
      "30000 test 97.5\n",
      "30000 pos 90.3667054715\n",
      "[[  1.14232473e-01   8.85767519e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.99134600e-01   8.65372655e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  4.80417088e-02   9.51958299e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.99972224e-01   2.77256204e-05   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99329567e-01   6.70410518e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  2.70141158e-02   9.72985923e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.99111950e-01   8.88061302e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  5.08298203e-02   9.49170232e-01   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99876738e-01   1.23193749e-04   1.00000000e+00   0.00000000e+00]\n",
      " [  5.72589561e-02   9.42741036e-01   0.00000000e+00   1.00000000e+00]\n",
      " [  9.96148944e-01   3.85101652e-03   1.00000000e+00   0.00000000e+00]\n",
      " [  1.00000000e+00   4.83957905e-08   1.00000000e+00   0.00000000e+00]\n",
      " [  9.87813056e-01   1.21869147e-02   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99999762e-01   2.63790497e-07   1.00000000e+00   0.00000000e+00]\n",
      " [  9.92103577e-01   7.89642893e-03   1.00000000e+00   0.00000000e+00]\n",
      " [  9.99975324e-01   2.46737018e-05   1.00000000e+00   0.00000000e+00]]\n",
      "31000 0.400699 93.75\n",
      "32000 0.310908 93.75\n",
      "33000 0.276772 93.75\n",
      "34000 0.225711 100.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-ca482534f214>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initialized valiables\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mdf_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_num_chunks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mdf_n\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#         df_c = data_cat_chunks.__next__()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yury/anaconda/envs/tensorflow/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yury/anaconda/envs/tensorflow/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mget_chunk\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m             \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 836\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yury/anaconda/envs/tensorflow/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yury/anaconda/envs/tensorflow/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    222\u001b[0m                                  dtype=dtype, copy=copy)\n\u001b[1;32m    223\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yury/anaconda/envs/tensorflow/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_init_dict\u001b[0;34m(self, data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_arrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_init_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yury/anaconda/envs/tensorflow/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_arrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[1;32m   5239\u001b[0m     \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_ensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5241\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yury/anaconda/envs/tensorflow/lib/python3.5/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_arrays\u001b[0;34m(arrays, names, axes)\u001b[0m\n\u001b[1;32m   3997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3998\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3999\u001b[0;31m         \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mform_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4000\u001b[0m         \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4001\u001b[0m         \u001b[0mmgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yury/anaconda/envs/tensorflow/lib/python3.5/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mform_blocks\u001b[0;34m(arrays, names, axes)\u001b[0m\n\u001b[1;32m   4019\u001b[0m     \u001b[0mextra_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4021\u001b[0;31m     \u001b[0mnames_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4022\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnames_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4023\u001b[0m         \u001b[0mnames_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yury/anaconda/envs/tensorflow/lib/python3.5/site-packages/pandas/indexes/base.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, data, dtype, copy, name, fastpath, tupleize_cols, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0;31m# other iterable of some kind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_asarray_tuplesafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \"\"\"\n",
      "\u001b[0;32m/Users/yury/anaconda/envs/tensorflow/lib/python3.5/site-packages/pandas/indexes/base.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, data, dtype, copy, name, fastpath, tupleize_cols, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnumeric\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFloat64Index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mFloat64Index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m             \u001b[0;32melif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_bool_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m                 \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yury/anaconda/envs/tensorflow/lib/python3.5/site-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mis_bool_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m   1696\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mis_bool_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1698\u001b[0;31m         \u001b[0mtipo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_dtype_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1699\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1700\u001b[0m         \u001b[0;31m# this isn't even a dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yury/anaconda/envs/tensorflow/lib/python3.5/site-packages/pandas/core/common.py\u001b[0m in \u001b[0;36m_get_dtype_type\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m   1510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1511\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_dtype_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1512\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1513\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marr_or_dtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1514\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "pos_size = 4\n",
    "batch_size_part = batch_size-pos_size\n",
    "num_steps = train_len//batch_size_part\n",
    "\n",
    "data_num_chunks = pd.read_csv('train_numeric.csv'      , index_col=0, chunksize=batch_size_part, skiprows=2001, dtype=np.float32, header=None)\n",
    "data_cat_chunks = pd.read_csv('train_categorical_c.csv', index_col=0, chunksize=batch_size_part, skiprows=2001, dtype=np.float32, header=None)\n",
    "\n",
    "\n",
    "print(num_steps)\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init_op = tf.initialize_all_variables()\n",
    "    saver = tf.train.Saver()\n",
    "    init_op.run()\n",
    "    print(\"Initialized valiables\")\n",
    "    for i in range(num_steps):\n",
    "        df_n = data_num_chunks.__next__()\n",
    "        df_n.fillna(0, inplace=True)\n",
    "#         df_c = data_cat_chunks.__next__()\n",
    "#         df_c = df_c.div(cat_features, axis='columns')\n",
    "#         df = pd.concat([df_c, df_n], axis=1)\n",
    "        df = df_n\n",
    "        X_ = df.values[:, :-1]\n",
    "        y_ = to_one_hot(df.values[:, -1], nlabels=2)\n",
    "\n",
    "        \n",
    "#         offset = (i * batch_size_part) % (y_train.shape[0] - batch_size_part)\n",
    "#         y_ = y_train[offset:(offset + batch_size_half)]\n",
    "\n",
    "        offset_pos = (i*pos_size) % (len(X_pos) - pos_size)\n",
    "#         add_idx = idx_pos[offset_pos:offset_pos+1]\n",
    "        y__ = y_pos_oh[offset_pos:offset_pos+pos_size]\n",
    "        X__ = X_pos[offset_pos:offset_pos+pos_size, :]\n",
    "\n",
    "        X_ = np.concatenate((X_, X__), axis=0)\n",
    "        y_ = np.concatenate((y_, y__), axis=0)\n",
    "\n",
    "        \n",
    "        p = np.random.permutation(batch_size)\n",
    "        X_ = X_[p]\n",
    "        y_ = y_[p]\n",
    "        \n",
    "        feed_dict = {X_tf : X_, y_tf : y_}\n",
    "        _, l, pred = sess.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (i < 100 and i%10 == 0) or i%1000 == 0:\n",
    "            print(i, l, accuracy(pred, y_ ))\n",
    "        if i>0 and i%10000 == 0:\n",
    "            print(i, \"test\", accuracy(test_prediction.eval(), y_valid_oh))\n",
    "            print(i, \"pos\", accuracy(pos_prediction.eval(), y_pos_oh))\n",
    "            print(np.c_[pred, y_])\n",
    "        if i>0 and i%30000 == 0:\n",
    "            save_path = saver.save(sess, \"bosch_{}.ckpt\".format(i))\n",
    "    save_path = saver.save(sess, \"bosch_end.ckpt\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data_cat = pd.read_csv(\"train_categorical.csv\")\n",
    "#data_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data_num_test = pd.read_csv(\"test_numeric.csv\")\n",
    "# data_num_test.fillna(0., inplace=True)\n",
    "# data_num_test.head()\n",
    "test_ids = np.genfromtxt ('test_numeric.csv', delimiter=\",\", skip_header=1, usecols = (0,), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1183748,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_test_real = data_num_test.iloc[:,1:].values.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data (?, 968)\n",
      "layer1 (?, 1024)\n",
      "layer2 (?, 512)\n",
      "layer3 (?, 256)\n",
      "layer6 (?, 2)\n",
      "Step 0 (50000, 969) 50000\n",
      "Step 1 (50000, 969) 100000\n",
      "Step 2 (50000, 969) 150000\n",
      "Step 3 (50000, 969) 200000\n",
      "Step 4 (50000, 969) 250000\n",
      "Step 5 (50000, 969) 300000\n",
      "Step 6 (50000, 969) 350000\n",
      "Step 7 (50000, 969) 400000\n",
      "Step 8 (50000, 969) 450000\n",
      "Step 9 (50000, 969) 500000\n",
      "Step 10 (50000, 969) 550000\n",
      "Step 11 (50000, 969) 600000\n",
      "Step 12 (50000, 969) 650000\n",
      "Step 13 (50000, 969) 700000\n",
      "Step 14 (50000, 969) 750000\n",
      "Step 15 (50000, 969) 800000\n",
      "Step 16 (50000, 969) 850000\n",
      "Step 17 (50000, 969) 900000\n",
      "Step 18 (50000, 969) 950000\n",
      "Step 19 (50000, 969) 1000000\n",
      "Step 20 (50000, 969) 1050000\n",
      "Step 21 (50000, 969) 1100000\n",
      "Step 22 (50000, 969) 1150000\n",
      "Step 23 (33748, 969) 1183748\n"
     ]
    }
   ],
   "source": [
    "data_real = pd.read_csv('test_numeric.csv', chunksize=50000, skiprows=1, dtype=np.float32, header=None)\n",
    "\n",
    "ans = np.array([])\n",
    "with tf.Session(graph = graph) as sess:\n",
    "    init_op = tf.initialize_all_variables()\n",
    "    saver = tf.train.Saver()\n",
    "    init_op.run()\n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, \"bosch_end.ckpt\")\n",
    "    \n",
    "    X_tf_test = tf.placeholder(tf.float32, shape=(None, features))\n",
    "    test_prediction = tf.nn.softmax(model(X_tf_test, train=False))\n",
    "    \n",
    "    i = -1\n",
    "    for data in data_real:\n",
    "        i+=1\n",
    "        data.fillna(0., inplace=True)\n",
    "        ans_ = sess.run(test_prediction, feed_dict = {X_tf_test:data.values[:, 1:]})\n",
    "        if i == 0:\n",
    "            ans = ans_\n",
    "        else:\n",
    "            ans = np.concatenate((ans, ans_), axis=0)\n",
    "        print(\"Step\", i, data.shape, len(ans))\n",
    "\n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1183748, 2), (1183748,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ans.shape, test_ids.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ans = np.argmax(ans, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = np.c_[test_ids, ans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('answer.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',')\n",
    "    writer.writerow([\"Id\",\"Response\"])\n",
    "    for i, a in enumerate(ans1):\n",
    "        writer.writerow([test_ids[i],a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('answer.csv',\n",
    "           result,\n",
    "           delimiter=',',\n",
    "           fmt=('%d', '%d'),\n",
    "           header='Id,Response',\n",
    "           comments='',\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
